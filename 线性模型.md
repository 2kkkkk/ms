[TOC]

# 线性回归

可以用梯度下降法来求解上述最优化问题的数值解，但是实际上该最优化问题可以通过最小二乘法获得解析
解。

当$X^TX$不是满秩矩阵。此时存在多个解析解，他们都能使得均方误差最小化。究竟选择哪个解作为输
出，由算法的偏好决定。

常见的做法是引入正则化项：L1 L2 L1_L2

L1为什么更容易获得稀疏解？https://www.zhihu.com/question/37096933?sort=created

# 广义线性模型

## 广义线性模型的函数定义

考虑单调可微函数g()，令$g(y)=\bold{w}\bold{x}+b$，这样得到的模型称作广义线性模型，其中函数g称作联系函数。

逻辑斯谛回归LR是广义线性模型在g(y)=ln(y)时的特例，即$ln y=\bold{w}\bold{x}+b$

## 广义线性模型的概率定义

如果给定$\bold{x}$和$\bold{w}$ 之后，y的条件概率分布$p(y|\bold{x};\bold{w})$服从指数分布族，则该模型称作广义线性模型。

常见分布的广义线性模型：高斯分布、伯努利分布均满足广义线性模型。LR属于伯努利分布的广义形式，而不是LR假设数据分布服从伯努利分布。
因为LR是判别式模型，哪来的假设数据服从什么分布

# LR

## 二分类模型

考虑二分类模型，考虑到$\bold{w}\bold{x}+b$取值是连续的，因此不能拟合二分类离散变量，可以考虑用它来拟合条件概率 ，$p(y=1|\bold{x})$因为概率的取值也是连续的。最理想的是单位阶跃函数，但是阶跃函数不满足单调可微的性质，对数几率函数( logistic function )就是这样的一个替代函数。这样的模型称作对数几率回归( logistic regression 或logit regression ）模型。虽然对数几率回归名字带有回归，但是它是一种分类的学习方法。其优点：

- 直接对分类的可能性进行建模，无需事先假设数据分布，这就避免了因为假设分布不准确带来的问题。
- 不仅预测出来类别，还得到了近似概率的预测，这对许多需要利用概率辅助决策的任务有用。
- 对数函数是任意阶可导的凸函数，有很好的数学性质，很多数值优化算法都能直接用于求取最优解。

## 参数估计

给定训练数据集，可以用极大似然估计法估计模型参数，从而得出模型。

似然函数为：$\prod_{i=1}^{N}[p(\mathbf{x_{i}})]^{y_i}[1-p(\mathbf{x_{i}})]^{1-y_i}$

为计算方便，取对数似然函数$\sum_{N}^{i=1}[y_ilogp(\mathbf{x_i})+(1-y_i)log(1-p(\mathbf{x_i}))]$

求似然函数最优化问题，通常用梯度下降法或者拟牛顿法来求解。

# 线性判别分析LDA

## 基本思想

训练时：给定训练样本集，设法将样例投影到某一条直线上，使得同类样例的投影点尽可能接近、异类
样例的投影点尽可能远离。要学习的就是这样的一条直线。
预测时：对新样本进行分类时，将其投影到学到的直线上，在根据投影点的位置来确定新样本的类别。

# 感知机

如果将感知机的损失函数定义成误分类点的中总数，则它不是w,b的连续可导函数，不容易优化。
因此，定义感知机的损失函数为误分类点到超平面S的总距离。