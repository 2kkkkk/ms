[TOC]

## GBDT

Gradient boosting is essentially a process of constructing an ensemble predictor by performing gradient descent in a functional space. It is backed by solid theoretical results that explain how strong predictors can be built by iteratively combining weaker models (base predictors) in a greedy manner [17].

GBDT可以分解为GB梯度提升+DT决策树，梯度提升属于boosting方法的一种，boosting方法可以概括为加法模型+前向分步算法，加法模型指的是通过组合多个弱学习器来得到一个强学习器，前向分步算法是通过分步迭代（stage-wise）的方式来构建模型，在迭代的每一步构建的弱学习器都是为了弥补已有模型的不足。梯度提升是在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。

（Boosting族算法的著名代表是AdaBoost，AdaBoost算法通过给已有模型预测错误的样本更高的权重，使得先前的学习器做错的训练样本在后续受到更多的关注的方式来弥补已有模型的不足。与AdaBoost算法不同，梯度提升方法在迭代的每一步构建一个能够沿着梯度最陡的方向降低损失（steepest-descent）的学习器来弥补已有模型的不足。经典的AdaBoost算法只能处理采用指数损失函数的二分类学习任务[2]，而梯度提升方法通过设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。另一方面，AdaBoost算法对异常点（outlier）比较敏感，而梯度提升算法通过引入bagging思想、加入正则项等方法能够有效地抵御训练数据中的噪音，具有更好的健壮性。） 

基于梯度提升算法的学习器叫做GBM(Gradient Boosting Machine)。理论上，GBM可以选择各种不同的学习算法作为基学习器。现实中，用得最多的基学习器是决策树。为什么梯度提升方法倾向于选择决策树（通常是CART树）作为基学习器呢？这与决策树算法自身的优点有很大的关系。决策树可以认为是if-then规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征，它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收bagging的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。现在主流的GBDT算法实现中这些方法基本上都有实现，因此GBDT算法的超参数还是比较多的，应用过程中需要精心调参，并用交叉验证的方法选择最佳参数。

### 加法模型

GBDT算法加法模型+前向分步算法的组合，GBDT可以看成是由K棵树组成的加法模型：
$$
\hat{y}_i=\sum_{k=1}^K f_k(x_i), f_k \in F \tag 0
$$
上述加法模型的目标函数定义为：$Obj=\sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$，其中$\Omega$表示决策树的复杂度，比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。

如何来学习加法模型呢？**通过前向分步算法学习加法模型**。从前往后，在每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，可以简化复杂度。 这一学习过程称之为Boosting。

那么，在每一步如何学习基函数及其系数呢？指导原则还是**最小化目标函数**。

在第t步，模型对$x_i$的预测为$\hat{y}_i^t= \hat{y}_i^{t-1} + f_t(x_i)$，其中$f_t(x_i)$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为：
$$
\begin{split}
Obj^{(t)} &= \sum_{i=1}^nl(y_i, \hat{y}_i^t) + \sum_{i=i}^t \Omega(f_i) \\
&=  \sum_{i=1}^n l\left(y_i, \hat{y}_i^{t-1} + f_t(x_i) \right) + \Omega(f_t) + constant
\end{split}\tag{1}
$$
假设损失函数为平方损失（square loss），则目标函数为：
$$
\begin{split}
Obj^{(t)} &= \sum_{i=1}^n \left(y_i - (\hat{y}_i^{t-1} + f_t(x_i)) \right)^2 + \Omega(f_t) + constant \\
&= \sum_{i=1}^n \left[2(\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \right] + \Omega(f_t) + constant
\end{split}\tag{2}
$$
其中$(y_i - \hat{y}_i^{t-1})$称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。（把$\left[2(\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \right]$ 看成是关于$f_t(x_i) $的一元二次函数，则当函数取最小值时$f_t(x_i)=-\frac{b}{2a}=-\frac{2(\hat{y}_i^{t-1} - y_i)}{2*1}=y_i - \hat{y}_i^{t-1} $）。



**gbdt中都是回归树，即分类问题转化成对概率的回归了**

## Xgboost

根据泰勒公式把函数$f(x+\Delta x)$在点x处二阶展开：
$$
f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac12 f''(x)\Delta x^2 \tag 3
$$
由等式(1)可知，目标函数是关于变量$\hat{y}_i^{t-1} + f_t(x_i)$的函数，若把变量$\hat{y}_i^{t-1}$看成是等式(3)中的x，把变量$f_t(x_i)$看成是等式(3)中的$\Delta x$，则第t步的目标函数可以写为：
$$
Obj^{(t)} = \sum_{i=1}^n \left[ l(y_i, \hat{y}_i^{t-1}) + g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) + constant \tag 4
$$
其中，$g_i$定义为损失函数的一阶导数，即$g_i=\partial_{\hat{y}^{t-1}}l(y_i,\hat{y}^{t-1})$；$h_i$定义为损失函数的二阶导数，即$h_i=\partial_{\hat{y}^{t-1}}^2l(y_i,\hat{y}^{t-1})$。
假设损失函数为平方损失函数，则$g_i=\partial_{\hat{y}^{t-1}}(\hat{y}^{t-1} - y_i)^2 = 2(\hat{y}^{t-1} - y_i)$，$h_i=\partial_{\hat{y}^{t-1}}^2(\hat{y}^{t-1} - y_i)^2 = 2$，把$g_i$和$h_i$代入等式(4)即得等式(2)。

由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4)中移除掉常量项，得：
$$
Obj^{(t)} \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) \tag 5
$$
由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数f(x)，从而根据加法模型等式(0)可得最终要学习的模型。



由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数，从而根据加法模型等式(0)可得最终要学习的模型。

### 重写优化函数

一颗生成好的决策树，假设其叶子节点个数为$T$，该决策树是由所有叶子节点对应的值组成的向量$w \in R^T$，以及一个把特征向量映射到叶子节点索引（Index）的函数$q:R^d \to \{1,2,\cdots,T\}$组成的。因此，决策树可以定义为$f_t(x)=w_{q(x)}$。

决策树的复杂度可以由正则项$\Omega(f_t)=\gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2$来定义，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。

定义集合$I_j=\{ i \vert q(x_i)=j \}$为所有被划分到叶子节点$j$的训练样本的集合。等式(5)可以根据树的叶子节点重新组织为T个独立的二次函数的和：
$$
\begin{split}
Obj^{(t)} &\approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) \\
&= \sum_{i=1}^n \left[ g_iw_{q(x_i)} + \frac12h_iw_{q(x_i)}^2 \right] + \gamma T + \frac12 \lambda \sum_{j=1}^T w_j^2 \\
&= \sum_{j=1}^T \left[(\sum_{i \in I_j}g_i)w_j + \frac12(\sum_{i \in I_j}h_i + \lambda)w_j^2 \right] + \gamma T
\end{split}\tag 6
$$
定义$G_j=\sum_{i \in I_j}g_i$，$H_j=\sum_{i \in I_j}h_i$，则等式(6)可写为：
$$
Obj^{(t)} = \sum_{j=1}^T \left[G_iw_j + \frac12(H_i + \lambda)w_j^2 \right] + \gamma T
$$
**先假设树的结构是固定的**，即函数$q(x)$确定，令函数$Obj^{(t)}$的一阶导数等于0，即可求得叶子节点$j$对应的值为：
$$
w_j^*=-\frac{G_j}{H_j+\lambda} \tag 7
$$
此时，目标函数的值为
$$
Obj = -\frac12 \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T \tag 8
$$
综上，为了便于理解，单颗决策树的学习过程可以大致描述为：

1. 枚举所有可能的树结构$q$
2. 用等式(8)为每个$q$计算其对应的分数$Obj$，分数越小说明对应的树结构越好
3. 根据上一步的结果，找到最佳的树结构，用等式(7)为树的每个叶子节点计算预测值

然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。

1. 从深度为0的树开始，对每个叶节点枚举所有的可用特征
2. 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）
3. .选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集
4. 回到第1步，递归执行到满足特定条件为止

在上述算法的第二步，样本排序的时间复杂度为$O(n \log n)$，假设公用K个特征，那么生成一颗深度为d的树的时间复杂度为$O(dKn\log n)$。具体实现可以进一步优化计算复杂度，比如可以缓存每个特征的排序结果等。

如何计算每次分裂的收益呢？假设当前节点记为$C$,分裂之后左孩子节点记为$L$，右孩子节点记为$R$，则该分裂获得的收益定义为当前节点的目标函数值减去左右两个孩子节点的目标函数值之和$Gain=Obj_C-Obj_L-Obj_R$，具体地，根据等式(8)可得：
$$
Gain=\frac12 \left[ \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma \tag 9
$$
其中，$-\gamma$项表示因为增加了树的复杂性（该分裂增加了一个叶子节点）带来的惩罚。等式(9)还可以用来计算输入特征的相对重要程度，具体可参考我的另外一篇博文[《Tree ensemble算法的特征重要度计算》](https://www.zybuluo.com/yxd/note/614495)。

最后，总结一下GBDT的学习算法：

1. 算法每次迭代生成一颗新的决策树
2. 在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数$g_i$和二阶导数$h_i$
3. .通过贪心策略生成新的决策树，通过等式(7)计算每个叶节点对应的预测值
4. 把新生成的决策树$f_t(x)$添加到模型中$\hat{y}_i^t = \hat{y}_i^{t-1} + f_t(x_i)$

通常在第四步，我们把模型更新公式替换为$\hat{y}_i^t = \hat{y}_i^{t-1} + \epsilon f_t(x_i)$，其中$\epsilon$称之为步长或者学习率。增加因子的目的是为了避免模型过拟合。

**参考资料：https://www.zybuluo.com/yxd/note/611571**

### 分裂点选取

贪婪算法可以的到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪婪算法这一缺点给出了近似最优解。 对于每个特征，只考察分位点可以减少计算复杂度。 该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。 在提出候选切分点时有两种策略： 

Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割； 

Local：每次分裂前将重新提出候选切分点。 直观上来看，Local 策略需要更多的计算步骤，而 Global 策略因为节点没有划分所以需要更多的候选点。

加权分位数缩略图 事实上， XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值 [公式] 作为样本的权重进行划分

## 问题

### **Q：boosting bagging**

boosting o	串行的方式训练弱学习器，各弱学习器之间有依赖。每一轮弱学习器根据前面模型的表现学习得到 bagging o	bagging是Bootstrap aggregating的意思，各分类器之间无强依赖，可以并行。 

•	boosting方法通过逐步聚焦分类器分错的样本，减少集成分类器的偏差 

•	Bagging采用分而治之的策略，通过对样本多次采样，分别训练多个模型，减少方差 o	

###  **Q：偏差与方差** 

模型输出值与真实值之间的差距(不同数据集下表现的平均值（期望）)

 模型在不同数据集下的输出值的方差 •	

Q：为什么决策树是常用的基分类器

 o	可以方便地将样本权重正和岛训练过程中，不需要使用过采样的方法来调整样本券种 

o	决策树的表达能力和繁华能力，可以通过调节树的层数来做折中 

o	数据样本扰动对决策树影响较大，因此不同子样本集生成的基分类器随机性就较大。这样的不稳定学习器更适合作为基分类器。

 o	插一句，神经网络也适合做基分类器

### **Q： GBDT主要的优缺点有** 

 GBDT的主要优点有： 

o	1) 可以灵活处理各种类型的数据，包括连续值和离散值。

 o	2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。 

o	3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 •	from百面机器学习： 

	预测阶段计算速度较快，树与树之间可以并行化计算 

	在分布稠密的数据机上，泛化能力和表达能力都比较好

 	具有较好的解释性和鲁棒性

 	能够自动发现特征质检的高阶关系 

	不需要做特殊预处理（比如归一化）

 GBDT的主要缺点有： 

o	由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 

o	在高维稀疏数据上，表现不如SVM或神经网络 

o	在处理文本分类特征问题上，相对其他模型优势不如在处理数值特征时明显

 o	训练过程需要串行，只能在决策树内部采用一些局部并行手段提高训练速度 

### **Q: XGB 分裂方式**

	分裂前后损失函数的差值为： 	XGB通过最大化这个差值作为准则来进行决策树的构建。通过遍历所有特征的取值，寻找时的损失函数前后相差最大时的分裂方式 	一次性求出最优树结构及叶子节点权重 w o	 •	

### **Q:对比原算法GBDT XGBoost主要从哪些方面做了优化：** 

o	一是算法本身的优化：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以直接很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。算法本身的优化是我们后面讨论的重点。 

o	二是算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。 

o	三是算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。Shrinkage（缩减），相当于学习速率（xgboost中的eta）。每次迭代，增加新的模型，在前面成上一个小于1的系数，降低优化的速度，每次走一小步逐步逼近最优模型比每次走一大步逼近更加容易避免过拟合现象；列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样（即每次的输入特征不是全部特征），不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。

###  **Q：XGBoost与GBDT有什么不同？** 

GBDT是机器学习算法，XGBoost是该算法的工程实现。

 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。 

GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。 

传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。 

传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。 

传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。 计算特征分裂点时，xgb用的是Gain公式。  



### **Q： 用gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用RandomForest的时候需要把树的深度调到15或更高？**

 A： 对于Bagging算法来说，我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance)，因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias），所以我们会采用深度很深甚至不剪枝的决策树。对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。? 

### **Q：和二分类相比 ，GBDT多分类是如何做的？损失函数是什么** 

一轮迭代中同时训练k棵树，最后用softmax求得概率。

### **Q:为什么XGBoost用二阶导数效果会好？** 

XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 



### **Q：XGB优缺点**

**优点：**

精度更高：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数； 

灵活性更强：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导； 

正则化：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合； 

Shrinkage（缩减）：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间； 

列抽样：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算； 

缺失值处理：XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度； 可以并行化操作：块结构可以很好的支持并行计算。 

**缺点 ：**

虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集； 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。

### **Q：GBDT和XGB缺点**

GBT 的缺点：在构建子决策树时为了获取分裂点，需要在所有特征上扫描所有的样本，从而获得最大的信息 增益。 当样本的数量很大，或者样本的特征很多时，效率非常低。 同时GBT 也无法使用类似mini batch 方式进行训练。

 xgboost 缺点： 每轮迭代都需要遍历整个数据集多次。 如果把整个训练集装载进内存，则限制了训练数据的大小。 如果不把整个训练集装载进内存，则反复读写训练数据会消耗非常大的IO 时间。 空间消耗大。预排序（ pre-sorted ）需要保存数据的feature 值，还需要保存feature 排序的结果 （如排序后的索引，为了后续的快速计算分割点）。因此需要消耗训练数据两倍的内存。 时间消耗大。为了获取分裂点，需要在所有特征上扫描所有的样本，从而获得最大的信息增益，时间消 耗大。 对cache 优化不友好，造成cache miss 。 预排序后， feature 对于梯度的访问是一种随机访问，并且不同feature 访问的顺序不同，无法 对cache 进行优化。 在每一层的树生长时，需要随机访问一个行索引到叶子索引的数组，并且不同feature 访问的顺 序也不同。

## Lightgbm

LightGBM 的思想：若减少训练样本的数量，或者减少样本的训练特征数量，则可以大幅度提高训练速度。 2. LightGBM 提出了两个策略： Gradient-based One-Side Sampling(GOSS) ： 基于梯度的采样。该方法用于减少训练样本的数量。 Exclusive Feature Bundling(EFB) ： 基于互斥特征的特征捆绑。该方法用于减少样本的特征。

我们刚刚分析了 XGBoost 的缺点，LightGBM 为了解决这些问题提出了以下几点解决方案：

### 单边梯度抽样算法



1. 直方图算法； 

2. 互斥特征捆绑算法； 

3. 基于最大深度的 Leaf-wise 的垂直生长算法； 

4. 类别特征最优分割； 

5. 特征并行和数据并行； 

6. 缓存优化。 

   

   GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算锅中只需关注梯度高的样本，极大的减少了计算量。 GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。 一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。 直方图算法的基本思想是将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。 我们知道特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）： •	内存占用更小：XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储索引，而 LightGBM 只需要用 8 位去存储直方图，相当于减少了 1/8； •	计算代价更小：计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度从   降低到   ，而我们知道   。 虽然将特征离散化后无法找到精确的分割点，可能会对模型的精度产生一定的影响，但较粗的分割也起到了正则化的效果，一定程度上降低了模型的方差。 2) 直方图加速 在构建叶节点的直方图时，我们还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。在实际操作过程中，我们还可以先计算直方图小的叶子节点，然后利用直方图作差来获得直方图大的叶子节点。 （父节点的直方图从分裂点砍一半 ，一半是左子节点的直方图，一半是右子节点的直方图） 2.1.3 互斥特征捆绑算法 高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。 针对这种想法，我们会遇到两个问题： 1.	哪些特征可以一起绑定？ 2.	特征绑定后，特征值如何确定？ 对于问题一：EFB 算法利用特征和特征间的关系构造一个加权无向图，并将其转换为图着色算法。我们知道图着色是个 NP-Hard 问题，故采用贪婪算法得到近似解，具体步骤如下： 1.	构造一个加权无向图，顶点是特征，边是两个特征间互斥程度； 2.	根据节点的度进行降序排序，度越大，与其他特征的冲突越大； 3.	遍历每个特征，将它分配给现有特征包，或者新建一个特征包，是的总体冲突最小。 对于问题二：论文给出特征合并算法，其关键在于原始特征能从合并的特征中分离出来。假设 Bundle 中有两个特征值，A 取值为 [0, 10]、B 取值为 [0, 20]，为了保证特征 A、B 的互斥性，我们可以给特征 B 添加一个偏移量转换为 [10, 30]，Bundle 后的特征其取值为 [0, 30]，这样便实现了特征合并。 2.1.4 带深度限制的 Leaf-wise 算法 在建树的过程中有两种策略： •	Level-wise：基于层进行生长，直到达到停止条件； •	Leaf-wise：每次分裂增益最大的叶子节点，直到达到停止条件。 XGBoost 采用 Level-wise 的增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，降低了计算量；LightGBM 采用 Leaf-wise 的增长策略减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂。

2.1.5 类别特征最优分割 大部分的机器学习算法都不能直接支持类别特征，一般都会对类别特征进行编码，然后再输入到模型中。常见的处理类别特征的方法为 one-hot 编码，但我们知道对于决策树来说并不推荐使用 one-hot 编码： 1.	会产生样本切分不平衡问题，切分增益会非常小。如，国籍切分后，会产生是否中国，是否美国等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0。这种划分的增益非常小：较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零； 2.	影响决策树学习：决策树依赖的是数据的统计信息，而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上统计信息不准确的，学习效果变差。本质是因为独热码编码之后的特征的表达能力较差的，特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败，最终该特征得到的重要性会比实际值低。 LightGBM 原生支持类别特征，采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分



### 与 XGBoost 的对比 

本节主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。 2.3.1 内存更小 1.	XGBoost 使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，将空间复杂度从   降低为   ，极大的减少了内存消耗； 2.	LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗； 3.	LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。 2.3.2 速度更快 1.	LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度； 2.	LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算； 3.	LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量； 4.	LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略； 5.	LightGBM 对缓存也进行了优化，增加了 Cache hit 的命中率。

4.2 缺点 可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合； Boosting族是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，模型的偏差（bias）会不断降低。由于LightGBM是基于偏差的算法，所以会对噪点较为敏感； 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去；

## 参考

GBDT[https://blog.csdn.net/wxn704414736/article/details/80563267](https://moa.hikvision.com/pcim/1.2.14/conversation?rnd=1615101030421#)

Lightgbm 直方图优化算法深入理解https://blog.csdn.net/anshuai_aw1/article/details/83040541