**Q:GBDT的优缺点？**

A：优点：

​      1. 可以灵活处理各种类型的数据，包括连续值和离散值。 

- 2.在分布稠密的数据集上，泛化能力和表达能力都比较好，具有较好的解释性

- 3.不需要做归一化

GBDT的主要缺点有： 

- 1.训练过程需要串行，无法并行训练多个弱分类器。

- 2.在高维稀疏数据上，表现不如SVM或神经网络 

**Q：用gbdt在在调参的时候把树的最大深度调成6就有很高的精度了。但是用RandomForest的时候需要把树的深度调到15或更高？**
对于Bagging算法来说，我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance)，因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias），所以我们会采用深度很深甚至不剪枝的决策树。对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。

**Q：和二分类相比 ，GBDT多分类是如何做的？损失函数是什么**
一轮迭代中同时训练k棵树，最后用softmax求得概率。

**Q：XGBoost与GBDT有什么不同？**

GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。

在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。

传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。
传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。
传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。

计算特征分裂点时，xgb用的是Gain公式。

**Q:为什么XGBoost用二阶导数效果会好？**
XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 

从泰勒公式展开角度，展开项越多，越能逼近原始函数

**Q：XGB如何处理缺失值 ？**

XGBoost没有假设缺失值一定进入左子树还是右子树，则是尝试通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向，这样处理起来更加的灵活和合理。

**Q：XGB做了哪些工程上的优化？**

**并行：**Boosting算法的弱学习器是没法并行迭代的，但是单个弱学习器里面最耗时的是决策树的分裂过程，XGBoost针对这个分裂做了比较大的**并行优化**。对于不同的特征的特征划分点，XGBoost分别在不同的线程中并行选择分裂的最大增益。 

**内存优化：**对训练的每个特征排序并且**以块的的结构存储在内存中**，方便后面迭代重复使用，减少计算量。计算量的减少参见上面第4节的算法流程，首先默认所有的样本都在右子树，然后从小到大迭代，依次放入左子树，并寻找最优的分裂点。这样做可以减少很多不必要的比较。 

**存储+IO**：通过设置合理的分块的大小，充分利用了CPU缓存进行读取加速（cache-aware access）。使得数据读取的速度更快。另外，通过将分块进行压缩（block compressoin）并存储到硬盘上，并且通过将分块分区到多个硬盘上实现了更大的IO。

**Q：XGB中的特征重要性是如何得到的？**

“weight”通过特征被选中作为分裂特征的计数来计算重要性 

“gain”和“total_gain”则通过分别计算特征被选中做分裂特征时带来的平均增益和总增益来计算重要性 

“cover”和 “total_cover”通过计算特征被选中做分裂时的平均样本覆盖度和总体样本覆盖度来来计算重要性。

**Q：LGB的3个亮点**

直方图算法、GOSS、EBF

**Q：LGB构建直方图时，如何确定每个bin的上下界？**

**Q：LGB直方图加速是怎么做的？**

构造叶子节点直方图的时候，可以直接用父节点的直方图与兄弟节点的直方图进行作差得到

**Q：LGB梯度单边采样是怎么做的？**

- - - 如果一个样本的梯度较小，证明这个样本训练的误差已经很小了，所以不需要计算了。我们在XGB的那篇文章中说过，GBDT的梯度算出来实际上就是残差，梯度小残差就小，所以该样本拟合较好，不需要去拟合他们了。
    - 这听起来仿佛很有道理，但问题是丢掉他们会改变数据的分布，还是无法避免信息损失，进而导致精度下降，所以LGB提出了一个很朴实无华且枯燥的方法进行优化。
    - LGB的优化方法是，在保留大梯度样本的同时，随机地保留一些小梯度样本，同时放大了小梯度样本带来的信息增益。

