[TOC]

# 预处理

## 缺失值

缺失值的处理有三种方法：

- 直接使用含有缺失值的数据。
  某些算法可以直接使用含有缺失值的情况，如决策树算法可以直接使用含有缺失值的数据。
  优点：直接使用原始数据，排除了人工处理缺失值带来的信息损失。
  缺点：只有少量的算法支持这种方式。

- 删除含有缺失值的数据。
  最简单的办法就是删除含有缺失值的样本。
  优点：简单、高效。
  缺点：如果样本中的缺失值较少，则直接丢弃样本会损失大量的有效信息。这是对信息的极大浪
  费。如果样本中包含大量的缺失值，只有少量的有效值，则该方法可以接受。

- 缺失值补全。
  用最可能的值来插补缺失值。这也是在实际工程中应用最广泛的技术。
  优点：保留了原始数据
  缺点：计算复杂，而且当插补的值估计不准确时，会对后续的模型引入额外的误差。

  均值插补 同类均值插补 建模预测 高维映射 多重插补 压缩感知及矩阵补全

## 编码

在决策树模型中，并不推荐对离散特征进行one-hot 。 主要有两个原因：1.切分不平衡的问题，此时且分增益会非常小。2.影响决策树的学习，决策树依赖的是数据的统计信息。而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上，统计信息是不准确的，学习效果变差。本质是因为独热码编码之后的特征的表达能力较差的。该特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败。最终该特征得到的重要性会比实际值低。

## 离散化

离散化用于将连续的数值属性转化为离散的数值属性。

离散化的常用方法是分桶。

分桶的数量和边界通常的确定，一般有两种方法：根据业务领域的经验来指定；根据模型指定。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界（ 类似这种超参数，比如桶的个数、边界，knn k的取值，cart树剪枝形成的子树序列选取，都可以通过交叉验证选择最优参数）。

选择分桶大小时，有一些经验指导：

- 分桶大小必须足够小，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。
  即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。
- 分桶大小必须足够大，使每个桶内都有足够的样本。
  如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。
- 每个桶内的样本尽量分布均匀。

在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列 0/1 的离散特
征。其优势有：

- 离散化之后得到的稀疏向量，内积乘法运算速度更快，计算结果方便存储。
- 离散化之后的特征对于异常数据具有很强的鲁棒性。
- 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于引入了非线
  性，提升模型的表达能力，增强拟合能力。
- 离散化之后，模型会更稳定。
- 特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险。

## 标准化、正则化

### 数据标准化

将样本的属性取值缩放到某个指定的范围。

数据标准化的两个原因：

1. **某些算法要求样本数据的属性取值具有零均值和单位方差。如PCA。 ** **所有依赖于样本距离的算法对于数据的数量级都非常敏感。**

（PCA(主成分分析)所对应的数学理论是SVD(矩阵的奇异值分解。而奇异值分解本身是完全不需要对矩阵中的元素做标准化或者去中心化的。但是对于机器学习，我们通常会对矩阵（也就是数据）的每一列先进行标准化。PCA通常是用于高维数据的降维，它可以将原来高维的数据投影到某个低维的空间上并使得其方差尽量大。如果数据其中某一特征（矩阵的某一列）的数值特别大，那么它在整个误差计算的比重上就很大，那么可以想象在投影到低维空间之后，为了使低秩分解逼近原数据，整个投影会去努力逼近最大的那一个特征，而忽略数值比较小的特征。因为在建模前我们并不知道每个特征的重要性，这很可能导致了大量的信息缺失。为了“公平”起见，防止过分捕捉某些数值大的特征，我们会对每个特征先进行标准化处理，使得它们的大小都在相同的范围内，然后再进行PCA。）

2. **样本不同属性具有不同量级时，消除数量级的影响。**
   数量级的差异将导致量级较大的属性占据主导地位。从而使得目标函数值仅依赖于该属性。
   数量级的差异将导致迭代收敛速度减慢。

**标准化方法**：

min-max 标准化

z-score 标准化：标准化之后，样本集的所有属性的均值都为 0，标准差均为 1。

训练集、验证集、测试集使用相同标准化参数，该参数的值都是从训练集中得到。

### 正则化

数据正则化是将样本的某个范数（如 范数）缩放到单位1。

正则化的过程是针对单个样本的，对每个样本将它缩放到单位范数。
标准化是针对单个属性的，需要用到所有样本在该属性上的值。

## 特征选择

进行特征选择的原因：

- 首先，在现实任务中经常会遇到维数灾难问题，这是由于属性过多造成的。
- 其次，去除不相关特征往往会降低学习任务的难度。

常见的特征选择方法大致可分为三类：过滤式filter 、包裹式wrapper 、嵌入式embedding 。

过滤式filter：过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。如卡方检验、F检验、互信息。

包裹式wrapper：包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。其目的就是为给定学习器选择最有利于其性能、量身定做的特征子集。

嵌入式特征选择是将特征选择与学习器训练过程融为一体，两者在同一个优化过程中完成的。即学习器训练过程中自动进行了特征选择。例如基于L1正则化的线性回归就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，二者同时完成。

## 类别不平衡问题

对于类别不平衡问题，常用的有4种方法：

1. 基于再缩放策略进行决策，称之为阈值移动threshold-moving 。再缩放虽然简单，但是由于“训练集是真实样本总体的无偏采样”这个假设往往不成立，所以无法基于训练集观
   测几率来推断出真实几率。
2. 直接对训练集里的反类样本进行欠采样undersampling 。常用方法是将反类划分成若干个集合供不同学习器使用，这样对每个学习器来看都是欠采样，但是全局来看并不会丢失重要信息。
3. 直接对训练集里的正类样本进行过采样oversampling 。过采样不能简单的对原始正类进行重复采样，否则会导致严重的过拟合。常见的有以下过采样策略：SMOTE等。
4. 对于正负样本极不平衡的场景，可以完全换一个不同的角度来看问题：将它看作一分类One Class Learning
   或者异常检测Novelty Detection 问题。

## 过拟合、欠拟合

过拟合overfitting ：选择的模型包含的参数过多，以至于该模型对于已知数据预测得很好，但是对于未知
数据预测的很差，使得训练误差和测试误差之间的差距太大。

过拟合无法避免，只能缓解。

欠拟合underfitting ：选择的模型包含的参数太少，以至于该模型对已知数据都预测的很差，使得训练误差
较大。

VC 维理论：训练误差与泛化误差之间差异的上界随着模型容量增长而增长，随着训练样本增多而下降 。

缓解过拟合的策略：

- 正则化。
- 数据集增强：通过人工规则产生虚假数据来创造更多的训练数据。
- 噪声注入：包括输入噪声注入、输出噪声注入、权重噪声注入。将噪声分别注入到输入/输出/权重参数
  中。
- 早停：当验证集上的误差没有进一步改善时，算法提前终止。



正则化 ：基于结构化风险最小化（ SRM ）策略的实现，其中 为正则化项。
在不同的问题中，正则化项可以有不同的形式：
回归问题中，损失函数是平方损失，正则化项是参数向量的L2范数（l2范数其实可以看成是一种模型参数的先验分布，高斯分布）
贝叶斯估计中，正则化项对应于模型的先验概率 。

## 偏差方差

偏差衡量的是偏离真实值的误差的期望。
方差衡量的是由于数据采样的随机性可能导致的估计值的波动。

如果模型存在高偏差，则通过以下策略可以缓解：
选择一个容量更大、更复杂的模型。
使用更先进的最优化算法。该策略通常在神经网络中使用。

如果模型存在高方差，则通过以下策略可以缓解：
增加更多的训练数据。它通过更多的训练样本来对模型参数增加约束，会降低模型容量。
如果有更多的训练数据，则一定会降低方差。
使用正则化。它通过正则化项来对模型参数增加约束，也会降低模型容量。
有时候更多的训练数据难以获取，只能使用正则化策略。

## 参数估计准则 

MLE MAP BAYES区别
首先三者都是参数估计的准则，也就是找到最优参数\theta
MLE:通过最大化似然函数p(X|\theta)，找出令似然函数最大化的那个\theta
MAP:通过最大化后验函数p(X|\theta)p(\theta)，找出令后验函数最大化的那个\theta
BAYES:不是找出单一的最优的\theta，而是求出\theta的分布， 用分布的期望作为\theta的估计值
https://zhuanlan.zhihu.com/p/37215276
https://www.cnblogs.com/jiangxinyang/p/9378535.html     (MLE MAP解释的还不错)



最大似然估计：最大化数据集 出现的概率。

**回归问题性能度量：**

当真实值的分布范围比较广时（如：年收入可以从 0 到非常大的数），如果使用MAE、MSE、RMSE 等误
差，这将使得模型更关注于那些真实标签值较大的样本。
而RMSLE 关注的是预测误差的比例，使得真实标签值较小的样本也同等重要。



## 降维

### PCA

物理意义为：通过线性组合原始特征，从而去掉一些冗余的或者不重要的特征、保留重要的特征。

这相当于在新的坐标系中：
任意一对特征之间的协方差为 0 。
单个特征的方差为 。
即：数据在每个维度上尽可能分散，且任意两个维度之间不相关。s

注意： PCA 推导过程中，并没有要求数据中心化；但是在推导协方差矩阵时，要求数据中心化。

注意：对验证集、测试集执行中心化操作时，中心向量必须从训练集计算而来。不能使用验证集的中心向
量，也不能用测试集的中心向量。

PCA 降维的准则有两个：
最近重构性：样本集中所有点，重构后的点距离原来的点的误差之和最小（就是前面介绍的内容）。
最大可分性：样本点在低维空间的投影尽可能分开。

可以证明，最近重构性就等价于最大可分性。

SVD 奇异值分解等价于PCA 主成分分析，核心都是求解 的特征值以及对应的单位特征向
量。

PCA 方法假设从高维空间到低维空间的映射是线性的，但是在不少现实任务中可能需要非线性映射才能找到
合适的低维空间来降维。
非线性降维的一种常用方法是基于核技巧对线性降维方法进行核化kernelized ， 如核主成分分析
Kernelized PCA:KPCA ，它是对PCA 的一种推广。
