[TOC]

## 高速公路卡口流量预测

### 数据情况，任务描述

数据集为高速公路卡口摄像头抓拍的车辆经过数据，预测给定时间截面未来一小时的车流量，评价指标为MAPE。

### 数据探索

1. 摄像头在同一时间点存在重复抓拍的情况，需要对车牌号进行去重。

2. 存在无车牌数据。

3. 通过对每小时的车流量进行可视化绘图，发现流量具有周期性，周期单位是天和周(一天内白天流量大，晚上流量小；一周内周一周五流量大，周三流量小）,且节假日出行高峰时，流量大于平常。可指导特征工程。

4. 可视化还发现流量曲线存在剧烈抖动的现象，怀疑可能存在异常值。

### 数据处理

首先需要从原始数据中生成车流量的统计值，选取的统计时间窗大小为5分钟，也就是统计每5分钟内的车流量，统计时对摄像头重复抓拍的情况进行了去重处理，并去掉了无车牌车辆。

from_unixtime(floor((unix_timestamp(pass_time) - 1577808000 - 1) / 300) * 300 + 300 + 1577808000, 'yyyy-MM-dd HH:mm:ss')

（为什么统计5分钟的流量？考虑到预测目标为当前时间截面偏移5分钟、15分钟、30分钟的流量，提供了参考。而5分钟可以作为一个最小时间窗，10分钟，30分钟的流量可以通过多个5分钟相加得到。通过统计发现，5分钟车流量数值不是特别大，也不会特别小，既可以捕捉到短时间内车流量的细微变化 ，也不至于由于车流量太小引入过多的噪声，且正好可以用于arima建模的数据要求。） 

**缺失值填充**（用0填充）（**哪来的缺失值**，5分钟内流量可能为0，需要将时间点补全）。

去掉节假日的数据（节假日出行高峰，相对于平常来说属于噪声，而最终要预测时间截面为非节假日）

异常值处理（1. 箱线图找出异常值并进行填充 。2. 对流量进行log变换，由于log函数的性质，可以找出流量偏少的异常数据，再进行异常值处理，填充时用同期（星期为周期，同小时，分钟）非异常值流量均值进行填充 3.直接进行削顶处理。） 

### 特征工程

特征工程：

时间类特征（周几，所在小时，时段，是否早晚高峰，是否是深夜休息时刻，一个月的第几天）

统计类特征：

1. 提取当前截面前一个小时每5分钟的流量，共12个点，及这12个点的统计特征（sum,max,min,mean,median,std,mean_decay）；（扩展：WT的，提取当前截面前12或24个小时每小时的流量及统计特征'mean', 'std', 'kurt', 'skew'

提取前7天每天当前时刻的小时流量，共7个点，及这7个点的统计特征；

提取前4周每周当前时刻的小时流量，共4个点，及这4个点的统计特征 建模预测

### 模型训练

（将各卡口数据放在一起训练 ），验证集取当前时间截面前一周内数据，xgb进行模型训练（网络调参）

LSTM模型：

数据需要归一化，单标签，多标签2种方式。

```python
# 1.训练集构建
for i in range(time_step, train_data.shape[0]):
    X_train.append(train_data.loc[i - time_step + 1: i,
                   feature_cols].values)  # X_train：列表，每个元素是二维数组，二维数组大小(time_step,features个数)
    y_train.append(
        train_data.loc[i - time_step + 1: i, label_col].values)  # y_train：列表，每个元素是二维数组，二维数组大小(time_step,1)
# 2.验证集构建
size = (len(valid_data) + time_step - 1) // time_step  # 有size个sample
for i in range(size - 1):
    X_valid.append(valid_data.loc[i * time_step:(i + 1) * time_step - 1, feature_cols].values)
    y_valid.append(valid_data.loc[i * time_step:(i + 1) * time_step - 1, label_col].values)
X_valid.append(valid_data.loc[len(valid_data) - time_step:, feature_cols].values)  # X_valid：列表，每个元素是二维数组
y_valid.append(valid_data.loc[len(valid_data) - time_step:, label_col].values)  # y_valid：列表，每个元素是二维数组
```



### 可能问的问题

#### Q：XGB回归的损失函数？

*objective*" = "*reg:linear*，MSE，平方损失，可能会问MSE对异常值敏感的问题。
当真实值的分布范围比较广时（如：年收入可以从 0 到非常大的数），如果使用MAE、MSE、RMSE 等误
差，这将使得模型更关注于那些真实标签值较大的样本。
而RMSLE 关注的是预测误差的比例，使得真实标签值较小的样本也同等重要。
当数据中存在**标签较大的异常值**时， RMSLE 能够降低这些异常值的影响。

#### Q：LSTM序列长度选择多少，如何确定？

24，验证集效果最好，且符合逻辑。



## 美团用户出行坐席预测

### 任务描述

通过预测用户出行所选择的坐席类别，对用户进行价格敏感度分群，进行个性化优惠券发放。最终目的为了拉新用户并节约成本。

### 建模方案

由于用户所选的坐席类别与出行距离具有相关性，不同用户长途短途之间选择的坐席并无可比性，例如，一个人选择飞机，另外一个人选择高铁二等座，并不能说明前者消费能力更强，很有可能前者是长途，而后者是短途旅行 。因此在给定用户短程、中程、长程的出行场景条件下，预测用户所选择的坐席类别

（短程、中程、长程的划分： 从选择硬卧出行的角度出发，普快火车时速为80-100km/h，将6个小时算做一晚，一晚的行程大概是600公里，为了进一步保证数据质量，将小于500km划为短程；进一步，将大于600km做划分，根据二等座和经济舱的出行距离分布（直方图和箱线图），以1200为界（大于1200km在二等座出行中占比较少，大于1200km在经济舱出行中占比仍然很大）大于600km小于1100km为中程，大于1200km小于4000km为长程 ）

从美团火车票机票订单表中选取下单时间在2018年4-6月的订单记录，根据出行距离划分为短程出行、中程出行和长程出行，然后再根据坐席类别生成label信息。 节假日是购票高峰期，可能存在票量不足的情况，因此过滤掉节假日期间的订单。 

训练集与测试集划分：用4，5月份的数据做训练集，6月份的数据作为测试集



### 特征工程

 特征提取： 	过去两周回家指数 	去年同期位移次数、最大位移距离 	过去一周火车票页面活跃天数 	过去一周各bu是否下单、是否异地下单 	相同geohash下单情况 	mt_user_hotel_coupon_features.predict 	用户一级品类偏好特征 	用户活跃度 	用户画像基础特征 	用户最近一周定位特征 	美团用户酒店相关特征（过去6个月、17个月消费次数，金额，活跃度，流失倾向，异地，消费价格区间，酒店星级，未使用的酒店优惠券） 	昨天的浏览及各bu购买行为 	敏感度相关特征 	使用红包比例 	点击券中心比例 	霸王餐比例 	酒店首次下单是否用红包 	大交通业务首次下单是否用红包 没有用户历史的坐席特征，更有利于预测新客 后面加入了历史出行特征 	历史出行特征 	过去一年购买飞机票次数 	过去一年购买商务舱/特等舱次数 	过去一年购买经济舱次数 	过去一年飞机出行总距离 	过去一年飞机出行总消费 	上一次飞机出行距今天数 	过去一年购买火车票次数 	过去一年购买商务座或特等座次数 	过去一年购买一等座或高级软卧或动卧次数 	过去一年购买二等座次数 	过去一年购买硬卧次数 	过去一年购买硬座或无座次数 	过去一年火车出行总距离 	过去一年火车出行总消费 	上一次火车出行距今天数 	选飞机比例 	选二等座比例 	选硬座无座比例 	平均每公里花费 	历史浏览特征 	过去一年境内游页面浏览次数 	过去一年境外游页面浏览次数 	过去一年境外游页面浏览占比 	过去一年火车票浏览次数 	过去一年飞机票浏览次数 	过去一年飞机票页面浏览占比 	过去一年特价飞机票页面浏览次数 	过去一年抢票页面浏览次数 	过去一年选择只看高铁次数 	过去一年选择只看高铁比例 	一级品类历史下单特征 	过去一年下单次数 	过去一年下单金额 	平均每单金额 	

### Kmeans聚类

将用户3个出行场景中9个坐席类别概率concat后，进行kmeans聚类，聚类为5类，因为优惠券有5个等级

### 效果

ABtest，成本降低了

### 可能问的问题

#### Q：XGB多分类损失函数？

交叉熵

#### Q：多分类类别不平衡问题

根据用户价格敏感度将用户分群，也就是9个类别能反映出用户的敏感度等级，因此引出一个问题，现在将长短途坐席划分成这9类是否是合理的，比如软卧类别，软卧的价格和二等座价格基本相等，而且类别数量很少，分类效果差。跟mentor讨论后，决定沿用初始的类别划分，原因是根据长短途每个类别的概率生成向量，对用户进行聚类，像软卧这种数量较少的类别，概率基本都很低，即这一维的方差很小，因此对聚类结果不会产生太大的影响。

#### Q：其他表问题

数据表权限问题、字段问题。想统计机票打折信息，但是相关表负责人说订单表已经没有折扣字段了，因此作废；想统计其他bu的详细的消费情况，考虑到这样需要用到的表较多，而且已经有一些用户消费情况的特征，因此简单统计了用户各bu的下单次数和金额

#### Q:KMeans算法相关问题



#### Q：如何优化？

想统计机票打折信息，加入打折特征

坐席类别划分，9类存在类别不均衡问题，可能影响聚类效果。其他聚类算法尝试

#### Q：其他聚类算法了解吗？



#### Q：如何评估算法有效性，ABtest如何进行，ABtest相关问题（左的wiki好像有）