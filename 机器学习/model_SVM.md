[TOC]

## 简介

对于多维空间中的二分类问题，SVM尝试寻找一个最优的超平面，使得两类样本分别分割在该超平面的两侧，且两侧距离超平面最近的样本点到超平面的距离被最大化。

SVM(支持向量机)最初是一种解决二分类的有监督学习算法，其目的在于：在给定两类样本的数据集的前提下，寻找一个将两类样本分隔开的超平面(separating hyperplane)，并且使得两类样本之间的边界间隔(margin)最大化。最终得到的超平面被称为决策边界(decision boundary)。

## 最大间隔超平面

超平面：$w^T \bold{x}+b=0$

函数间隔：$y^i(w^T \bold{x}^i+b)$

几何间隔：$\frac{y^i(w^T \bold{x}^i+b)}{||w||}$

根据简介中SVM的思想，最大间隔超平面需要满足两个约束条件：（1）能够将正负样本正确分类，或者说样本点到超平面的函数间隔大于零；（2）离超平面最近的样本点与超平面的几何间隔最大。

第一个条件的数学表达式为$y^i(w^T \bold{x}^i+b)>0$

第二个条件的数学表达式为
$$
\underset{w,b}{max} \ \underset{i}{min}\frac{y^i(w^T \bold{x}^i+b)}{||w||}=\underset{w,b}{max}\frac{1}{||w||} \underset{i}{min}\ {y^i(w^T \bold{x}^i+b)},i=1,2,3...,m
$$
结合这两个约束条件，我们可以得到求解最大间隔超平面的求解公式：
$$
\underset{w,b}{max}\frac{1}{||w||} \underset{i}{min}\ {y^i(w^T \bold{x}^i+b)},i=1,2,3...,m\\ \tag1
subject\ to\ y^i(w^T \bold{x}^i+b)>0
$$
需要对上式进行简化以求解。

对w,b进行等比例变化，变化后的超平面与变化前的超平面相同，**可以说某一确定的超平面对应无数个w和b**。**如果我们不将这个缩放对超平面表达式的影响消除的话，那么就算用上式能够求解，解出的最优超平面也对应有无数个w和b，解不唯一**。

若是我们要求某一固定的超平面，其对应的w，b是唯一的，那么就要需要添加限制条件。限制条件可以有很多种，选取其中一种即可。如向量w的模长为1，也就是说，你只能用模长为1的法向量来表示超平面，这样就消除了等比例缩放的影响。但是，添加这样一个限制条件，虽然满足要求，但并不能对式(1)进行简化，所以我们需要换一种思路来添加限制条件。

SVM选用的限制条件为：${min}\ {y^i(w^T \bold{x}^i+b)}=1,i=1,2,3...,m$，**即距离超平面的最小函数间隔为1**，这个式子将样本点和超平面联系起来，用以确定超平面的唯一表示参数w,b。

仔细思考一下这个限制条件是否合适。当超平面固定，我们一定能找到，所有样本点中与超平面函数间隔最小的样本点，不妨将这个解记为$x^j$。在$x^j$的数据值和超平面固定的条件下，令$y^j(w^T \bold{x}^j+b)=1$就相当于自适应缩放固定超平面的w,b,以使得最终w,b代入$y^j(w^T \bold{x}^j+b)$结果为1。**实际上，超平面固定时，在令最小函数间隔为1的条件下，只有唯一的w,b能够满足条件。**

同时，需要注意的是**w,b等比例缩放并不改变几何间隔**

接下来我们来看看，添加这个限制条件对于式(1的改进。首先最优化目标函数变为：
$$
\underset{w,b}{max}\frac{1}{||w||}
$$
然后约束条件也要变，条件${min}\ {y^i(w^T \bold{x}^i+b)}=1,i=1,2,3...,m$等价于$ {y^i(w^T \bold{x}^i+b)}\geqslant 1,i=1,2,3...,m$，将其与原条件$ {y^i(w^T \bold{x}^i+b)}>0,i=1,2,3...,m$综合一下即为$ {y^i(w^T \bold{x}^i+b)}\geqslant 1,i=1,2,3...,m$。

于是我们得到求解最大间隔超平面的最终表达式：
$$
\underset{w,b}{max}\frac{1}{||w||}\\
subject\ to\ {y^i(w^T \bold{x}^i+b)}\geqslant 1,i=1,2,3...,m \tag2
$$
在满足条件的前提下，改变w，b，超平面位置也会随之发生变化。这个时候，才是真正的迭代求最优解。

**参考：**https://zhuanlan.zhihu.com/p/144951818

## 





## 问题

SVM中的正则化和损失是什么？https://www.zhihu.com/question/30230784/answer/47837249



https://zhuanlan.zhihu.com/p/76946313

https://zhuanlan.zhihu.com/p/43827793

https://blog.csdn.net/v_JULY_v/article/details/7624837